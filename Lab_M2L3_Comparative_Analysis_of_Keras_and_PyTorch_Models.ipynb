{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf6b95-f975-42df-b63b-f59078b5cd51",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Comparative Analysis of Keras and PyTorch Models </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f6f80-948d-4eb5-bd73-e328fc644dda",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b5aad",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "\n",
    "1. Prepare data, load and evaluate Keras model.\n",
    "2. Prepare data, load and evaluate PyTorch model.\n",
    "3. Compute multiple performance metrics including accuracy, precision, recall, and f1-score.\n",
    "4. Visualize receiver operating characteristic (ROC) curves.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab295fd-5b27-4446-8cd2-9e78fff705ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4639ce32-38b5-410f-9c8f-7186738b8a38",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will compare the performance of the Keras-based and the PyTorch based convolutional neural network (CNN) models using various evaluation metrics.  Common metrics include:\n",
    "\n",
    "- **Accuracy**: Measures how often the model is correct overall. A higher value means more total predictions are correct.\n",
    "\n",
    "- **Precision**: Measures how many predicted positives are actually correct. A higher value means fewer false positives (incorrectly predicted positives).\n",
    "\n",
    "- **Recall**: Measures how many real positives the model finds. A higher value means fewer false negatives (missed positive cases).\n",
    "\n",
    "- **F1 Score**: Tells us about the balance between precision and recall. A higher value means a better trade-off between precision and recall.\n",
    "\n",
    "- **ROC-AUC**: Measures the model’s ability to distinguish classes. A higher value reflects a model that can better distinguish between classes at all probability thresholds.\n",
    "\n",
    "\n",
    "For all these metrics, the model should aim for values as close to 1.0 (or 100%) as possible. Lower values indicate poorer model performance. There are exceptions for some metrics in other settings (like various loss functions, where lower is better), but for these standard classification metrics, higher is always better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6219bc4-772b-4a74-88b1-4efe292b53b7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "    \n",
    "1. [Data download and extraction](#Data-download-and-extraction)\n",
    "2. [Package installation](#Package-installation)\n",
    "3. [Library imports and setup](#Library-imports-and-setup)\n",
    "4. [Evaluation metrics](#Evaluation-metrics)\n",
    "    1. [Accuracy](#1.-Accuracy)\n",
    "    2. [Precision](#2.-Precision)\n",
    "    3. [Recall](#3.-Recall-(sensitivity-or-true-positive-rate))\n",
    "    4. [F1 score](#4.-F1-score)\n",
    "    5. [Confusion matrix](#5.-Confusion-matrix)\n",
    "    6. [ROC-AUC](#6.-ROC-AUC-(Receiver-operating-characteristic---Area-under-curve))\n",
    "6. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "7. [Model paths and download](#Model-paths-and-download)\n",
    "8. [Dataset path and parameters](#Dataset-path-and-parameters)\n",
    "9. [PyTorch model evaluation and prediction](#PyTorch-model-evaluation-and-prediction)\n",
    "10. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "11. [Keras model evaluation and prediction](#Keras-model-evaluation-and-prediction)\n",
    "12. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "13. [ROC curve plotting](#ROC-curve-plotting)\n",
    "14. [Comparing model performance](#Comparing-model-performance)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0caef99bb4d4a4c92e69c5e6fb6e909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb4315e55a64d92bced61a30c89a7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.2 ms, sys: 26.4 ms, total: 70.7 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28739c-8b88-4fc1-901b-7a3c24f8dd4f",
   "metadata": {},
   "source": [
    "### Install PyTorch library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a97ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.7.0\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch==2.7.0)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.1.5)\n",
      "Collecting fsspec (from torch==2.7.0)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.7.0 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 4.44 s, sys: 1.14 s, total: 5.58 s\n",
      "Wall time: 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348399a-ee14-4050-a752-96f367f21b12",
   "metadata": {},
   "source": [
    "### Install PyTorch helper libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a20f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision==0.22 in /opt/conda/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (11.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 15.2 ms, sys: 10.5 ms, total: 25.7 ms\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66b191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19 in /opt/conda/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.11.3)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (14.1.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 25 ms, sys: 12.6 ms, total: 37.6 ms\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c8f13-46b3-4e8e-a988-3af92750dfee",
   "metadata": {},
   "source": [
    "### Install SkLearn library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9829348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.7.0\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn==1.7.0)\n",
      "  Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.7.0)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.7.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m190.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.0 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 122 ms, sys: 32 ms, total: 154 ms\n",
      "Wall time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 591 ms, sys: 199 ms, total: 790 ms\n",
      "Wall time: 919 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b7cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 2.69 s, sys: 688 ms, total: 3.38 s\n",
      "Wall time: 7.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99804321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756634597.933369     327 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756634597.940013     327 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756634597.959183     327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756634597.959221     327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756634597.959222     327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756634597.959224     327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 2.5 s, sys: 454 ms, total: 2.95 s\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae515f",
   "metadata": {},
   "source": [
    "## Evaluation metrics \n",
    "\n",
    "The following metrics are used for evaluation of various AI/ML models:\n",
    "    \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic - Area Under Curve (ROC-AUC)\n",
    "\n",
    "You can read about their calculation methods and their significance for model performance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e4b91-1b94-4563-a099-6a811a328f70",
   "metadata": {},
   "source": [
    "### 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "Accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In other words, it measures how often the classifier is correct overall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\\]\n",
    "\n",
    "- TP: True positives (correctly predicted positive cases)\n",
    "- TN: True negatives (correctly predicted negative cases)\n",
    "- FP: False positives (incorrectly predicted positive cases)\n",
    "- FN: False negatives (incorrectly predicted negative cases)\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "Accuracy is intuitive and easy to interpret, making it a common first metric for model evaluation. However, it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other). This is because a model can achieve high accuracy by simply predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879be2c6-dd4a-4efb-be7a-32a21ec3ba9d",
   "metadata": {},
   "source": [
    "### 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of positive predictions that are actually correct. It answers the question: \"Of all the samples that the model predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Precision is crucial when the cost of a false positive is high. For example, in medical diagnosis, predicting a disease when it's not present (false positive) can lead to unnecessary treatments. In land classification, high precision means that when the model predicts a tile as agricultural, it is likely correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be151b6-8f48-4ec2-9cc9-0e7cf24feac5",
   "metadata": {},
   "source": [
    "### 3. Recall (sensitivity or true positive rate)\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of actual positive cases that were correctly identified by the model. It answers: \"Of all the true positive samples, how many did the model identify?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Recall is important when the cost of missing a positive case (false negative) is high. In land classification, high recall means the model is good at finding all the agricultural land, even if it sometimes mislabels non-agricultural land as agricultural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eba49-ce14-4cef-9fb0-45c8a2a7931c",
   "metadata": {},
   "source": [
    "### 4. F1 score\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is especially useful when you need to find an equilibrium between precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "F1 = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "The F1 score is especially valuable when the class distribution is uneven or when both false positives and false negatives are important. It penalizes extreme values, so a model with high precision but low recall (or vice versa) will have a lower F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48236-1003-4888-b84c-bcd2b5b385e0",
   "metadata": {},
   "source": [
    "### 5. Confusion matrix\n",
    "\n",
    "**Definition:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification algorithm. It displays the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "|               | Predicted positive | Predicted negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Actual positive | True positive (TP) | False negative (FN) |\n",
    "| Actual negative | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**Significance:**\n",
    "The confusion matrix provides a detailed breakdown of model errors and successes, helping you understand not just how often the model is right, but *how* it is wrong. This is crucial for diagnosing issues like class imbalance or systematic misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b5a8b-6836-4fb7-86db-36c6b8efb675",
   "metadata": {},
   "source": [
    "### 6. ROC-AUC (Receiver operating characteristic - Area under curve)\n",
    "\n",
    "**Definition:**\n",
    "ROC-AUC measures the model's ability to distinguish between classes across all possible classification thresholds. The ROC curve plots the true positive rate (recall) against the false positive rate at various thresholds. The AUC (area under the curve) summarizes this performance in a single value between 0 and 1.\n",
    "\n",
    "**Significance:**\n",
    "A model with an ROC-AUC of 1.0 perfectly distinguishes between classes, while a value of 0.5 suggests random guessing. ROC-AUC is especially useful for imbalanced datasets and when you care about the ranking of predictions rather than their absolute values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51947c1-a446-45f4-83d7-b9d8472847e4",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ab94b1-7072-4d1f-8838-cf14c99a17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 μs, sys: 0 ns, total: 21 μs\n",
      "Wall time: 24.1 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be04a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras...\n",
      "Successfully downloaded './ai-capstone-keras-best-model-model_downloaded.keras'.\n",
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth...\n",
      "Successfully downloaded './ai_capstone_pytorch_best_model_state_dict_downloaded.pth'.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6a821",
   "metadata": {},
   "source": [
    "## Dataset path and parameters\n",
    "\n",
    "Here, for downstream processing, you define \n",
    "1. the dataset directory path\n",
    "2. define image dimensions\n",
    "3. number of channels\n",
    "4. batch size\n",
    "5. number of classes\n",
    "6. class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87cfea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2bc67",
   "metadata": {},
   "source": [
    "## Keras model evaluation and prediction\n",
    "\n",
    "In this cell, you will:\n",
    "- Use `ImageDataGenerator` to rescale images.\n",
    "- Load test images from the dataset directory.\n",
    "- Load the saved Keras model using `tf.keras.models.load_model`.\n",
    "- Run predictions on the test set, collect predicted probabilities, predicted classes, and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fd7f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 2 classes.\n",
      "Number of Steps: 47 with batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|██████████| 47/47 [01:54<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 5.51 s, total: 1min 35s\n",
      "Wall time: 1min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "steps = int(np.ceil(prediction_generator.samples / prediction_generator.batch_size))\n",
    "batch_size = int(prediction_generator.batch_size)\n",
    "print(f\"Number of Steps: {steps} with batch size: {batch_size}\")\n",
    "\n",
    "all_preds_keras = []\n",
    "all_probs_keras = []\n",
    "all_labels_keras = []\n",
    "\n",
    "for step_idx, step in enumerate(tqdm(range(steps), desc=\"Steps\")):\n",
    "    images, labels = next(prediction_generator)\n",
    "    preds = keras_model.predict(images, verbose='0')\n",
    "    all_probs_keras.extend(preds)\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    all_preds_keras.extend(preds)\n",
    "    all_labels_keras.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7cb84-c131-41b7-9bed-6aecb8758974",
   "metadata": {},
   "source": [
    "### Question: What does the code **`preds > 0.5`** in line `preds = (preds > 0.5).astype(int).flatten()` do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407213e-3558-4f82-8db1-7f9810347324",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "<!--\n",
    "The code preds > 0.5 converts the model's continuous probability outputs into distinct binary class labels.\n",
    "\n",
    "How It Works\n",
    "For this binary classification task, the Keras model outputs a single probability score for each image, a value between 0 and 1. This score represents the model's confidence that the image belongs to the positive class (\"agri\").\n",
    "\n",
    "The expression preds > 0.5 acts as a decision threshold:\n",
    "\n",
    "If a probability in preds is greater than 0.5, the expression evaluates to True.\n",
    "\n",
    "If a probability is 0.5 or less, it evaluates to False.\n",
    "\n",
    "The subsequent .astype(int) method then converts these boolean True/False values into integers, where True becomes 1 (the \"agri\" class) and False becomes 0 (the \"non-agri\" class).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008b4c1-2d7d-4a93-9714-a687b3e29a5e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It converts all predictions greater than 0.5 to True or assign to class 1. Rest of the predictions are False, assigned to class 0\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971389d2-f67f-4c21-a79a-2db089082422",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec013b-f37f-4231-a33f-952650170d71",
   "metadata": {},
   "source": [
    "### Task 1: Print the performance metrics for the Keras model using `print_metrics` function\n",
    "\n",
    "Print various performance metrics for the **Keras** model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54ab980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for the \u001b[1mKeras Model\u001b[0m\n",
      "Accuracy:  0.9925\n",
      "ROC-AUC:   1.0000\n",
      "Loss:      0.0247\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    non-agri     0.9852    1.0000    0.9926      3000\n",
      "        agri     1.0000    0.9850    0.9924      3000\n",
      "\n",
      "    accuracy                         0.9925      6000\n",
      "   macro avg     0.9926    0.9925    0.9925      6000\n",
      "weighted avg     0.9926    0.9925    0.9925      6000\n",
      "\n",
      "========= Confusion Matrix =========\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Please use the space below to write your answer\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2e854-e1c3-4d08-a0a3-eaa09340a389",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b613b4-911e-485c-9de7-f98b3c02e068",
   "metadata": {},
   "source": [
    "### Question: What is the significance of `f1 score`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7120368-d318-49cc-8db2-7f87b1eacb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "<!-- \n",
    "The F1 score's significance lies in its ability to provide a single metric that balances both precision and recall, making it especially useful when the costs of false positives and false negatives are both high.\n",
    "-->  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e571bbc-2708-4db9-b9ca-6573c0bd095d",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It is useful when both false positives and false negatives are important\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c22e",
   "metadata": {},
   "source": [
    "## PyTorch model evaluation and prediction\n",
    "\n",
    "In this cell, you:\n",
    "- Set device for inference (GPU if available).\n",
    "- Define data transformations including resizing, normalization.\n",
    "- Load the dataset using `ImageFolder` and prepares a DataLoader.\n",
    "- Define the CNN architecture matching the saved state dict.\n",
    "- Load model weights.\n",
    "- Run inference on the test set, collecting predicted classes, probabilities, and true labels for metric calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inference on cpu\n",
      "Created model, now loading the weights from saved model state dict\n",
      "Loaded model state dict, now getting predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Processing inference on {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "    nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "    nn.Linear(2048, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(\"Created model, now loading the weights from saved model state dict\")\n",
    "model.load_state_dict(torch.load(pytorch_state_dict_path))\n",
    "print(\"Loaded model state dict, now getting predictions\")\n",
    "\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc86e4a-c572-439b-b23a-5e272299caf9",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119d485-8fbd-4c4e-9405-505369288e48",
   "metadata": {},
   "source": [
    "### Task 2: Print the performance metrics for the PyTorch model using `print_metrics`\n",
    "\n",
    "Print various performance metrics for the PyTorch model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c22ac-e1fa-489e-b886-8de9a7ff3848",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0a22b-3cf1-4543-b012-77c192042eff",
   "metadata": {},
   "source": [
    "### Question: What are the total number of false negatives in the `confusion matrix` in the PyTorch model evaluated above? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4409dd6-6523-4a20-ad84-2f3a72ab67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "<!-- ## Please use the space below to write your answer  \n",
    "A false negative (FN) occurs when the model incorrectly predicts the negative class. In the context of this lab:\n",
    "\n",
    "Actual Class: The image is agricultural land (\"agri\").\n",
    "\n",
    "Predicted Class: The model classifies it as non-agricultural land (\"non-agri\").\n",
    "\n",
    "Therefore, the 5 false negatives represent 5 instances where the PyTorch model failed to identify actual agricultural land. In the confusion matrix plot, this number would appear in the cell corresponding to the \"agri\" row (True Label) and the \"non-agri\" column (Predicted Label).\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f876e-6165-4553-90f3-fcdd9bd1d056",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"Total Flase negatives are 5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebfcf7",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792448",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e6e57-9764-44a3-8566-cef177c1e684",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871167c-5e80-4f64-bc89-106ffc63935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "\n",
    "mertics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in mertics_list:\n",
    "    print(\"{:<18} {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5367255-39c2-473d-95bb-393e8baf6622",
   "metadata": {},
   "source": [
    "### Metric analysis\n",
    "\n",
    "The metrics for the pre-trained Keras and PyTorch models for evaluating the provided dataset are:\n",
    "\n",
    "- **Accuracy**\n",
    "    1. Keras: 0.9925\n",
    "    2. PyTorch: 0.9988\n",
    "    \n",
    "    ===> Both models achieve exceptional accuracy, but the **PyTorch model makes fewer mistakes**.\n",
    "\n",
    "- **Precision**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 0.9983\n",
    "\n",
    "    ===> The **Keras** model perfectly **avoids false positives**, whereas the PyTorch model is slightly less perfect but still excellent.\n",
    "\n",
    "- **Recall**\n",
    "    1. Keras: 0.9850\n",
    "    2. PyTorch: 0.9993\n",
    "    \n",
    "    ===> The **PyTorch** model is marginally better at **identifying all true positives**, capturing nearly all actual positive cases, while the Keras model misses a few.\n",
    "\n",
    "- **F1 Score**\n",
    "    1. PyTorch: 0.9988\n",
    "    2. Keras: 0.9924\n",
    "    \n",
    "    ===> The F1 score, which balances precision and recall, favors the **PyTorch** model thanks to its **stronger recall**.\n",
    "\n",
    "- **ROC-AUC**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 1.0000\n",
    "    \n",
    "    ===> Both models reach maximum possible **discrimination between classes**, indicating outstanding capability for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30912d-7ebe-4a8f-b5f1-d14fb1a5a12f",
   "metadata": {},
   "source": [
    "### **Model comparison: Key insights**\n",
    "\n",
    "\n",
    "**PyTorch model strengths**\n",
    "\n",
    " - Achieves the highest scores in accuracy, recall, and F1, indicating extremely robust overall performance and near-perfect classification of positive cases\n",
    "- ROC-AUC of 1.0 shows perfect class separability\n",
    "\n",
    "\n",
    "**Keras model strengths**\n",
    "\n",
    "- Displays almost perfect precision every positive prediction made is correct\n",
    "- Also achieves perfect ROC-AUC, indicating outstanding discrimination ability\n",
    "\n",
    "\n",
    "**Common strength**\n",
    "\n",
    "- Both models deliver flawless ROC-AUC, suggesting both are highly effective for this classification task\n",
    "\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "Based on the scores from the uploaded pre-trained models:\n",
    "\n",
    "- The PyTorch model is preferable for applications where missing any positive instances is costly (higher recall)\n",
    "- The Keras model is optimal for scenarios where making any false positive error is unacceptable (higher precision).\n",
    "\n",
    "\n",
    "**Next**\n",
    "\n",
    "- Analyze the confusion matrices to investigate the errors.\n",
    "- Monitor real-world performance, as even marginal differences can become important in high-impact applications. \n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Both models excel in all evaluated metrics and would be highly reliable in production. The PyTorch model demonstrates a modest edge in recall and F1 score, while the Keras model maximizes precision. The choice between models should ultimately reflect the specific requirements and risk tolerance of your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb79672-3cdf-47b4-ae97-43d53965d480",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully evaluated and compared two deep learning models, one using Keras and the other using the PyTorch framework.\n",
    "\n",
    "You learnt about a comprehensive workflow for comparing Keras and PyTorch models on the same dataset and got hands-on experience on:\n",
    "- data preparation\n",
    "- model loading\n",
    "- predicting dataset\n",
    "- metric computation\n",
    "- ROC visualization\n",
    "- Model performance comparison\n",
    "\n",
    "Using these framework independent metrics, you now know how to evaluate different models for their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8538ff197cd5769fae848cba4dd9cad3b9711ff18ba9768a8bf3f83e305d573f"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
