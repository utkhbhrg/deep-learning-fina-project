{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Implement and Test a PyTorch-Based Classifier</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objective</h2><ul>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "1. Create a PyTorch-based CNN model for classification.\n",
    "2. Train this model for the classification of agricultural and non-agricultural land.\n",
    "3. Evaluate the performance of this CNN model.\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **PyTorch-based Convolutional Neural Network (CNN)** for image classification, for agricultural images in your case. You will cover the following:\n",
    "1. *Data preparation*\n",
    "2. *Model architecture* definition\n",
    "3. *Training*, and\n",
    "4.  Model *performance analysis*\n",
    "\n",
    "The goal is to classify satellite images into two categories: 'agricultural' and 'non-agricultural'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Ensuring repeatability in PyTorch](#Ensuring-repeatability-in-PyTorch)\n",
    "4. [Defining hyperparameters and device](#Defining-hyperparameters-and-device)\n",
    "5. [The data pipeline](#The-data-pipeline)\n",
    "6. [Defining the model](#Defining-the-model)\n",
    "7. [Training and validation](#Training-and-validation)\n",
    "8. [Save and download the trained model weights](#Save-and-download-the-trained-model-weights)\n",
    "9. [Visualizing training history](#Visualizing-training-history)\n",
    "10. [Final model evaluation](#Final-model-evaluation)\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them, it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where you try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library installation - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 39.7 ms, total: 149 ms\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the above libraries installed properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the `PyTorch` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.7.0\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch==2.7.0)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.1.5)\n",
      "Collecting fsspec (from torch==2.7.0)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/393.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torchvision` library installation\n",
    "\n",
    "Install the `torchvision` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.22\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (11.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 307 ms, sys: 67.1 ms, total: 374 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-learn` library installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 154 ms, sys: 42.6 ms, total: 196 ms\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- AI LIBRARY IMPORTS ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Data Extraction Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Defining dataset URL\n",
    "\n",
    "\n",
    "Let's define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (like S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1. Download and extract data from the cloud using the `skillsnetwork.prepare` method\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats like gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (e.g., 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring repeatability in PyTorch\n",
    "\n",
    "To achieve reproducible results when you train a CNN in PyTorch, you must follow three steps:\n",
    "\n",
    "1.  Define a helper called `set_seed` that seeds every random-number generator and configures cuDNN for deterministic kernels.\n",
    "2.  Call `set_seed()` *once* at the top of your script/notebook to lock in the seed for the main process.\n",
    "3.  Provide a `worker_init_fn` so each `DataLoader` worker starts from a reproducible seed as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the `set_seed` function\n",
    "What the `set_seed` function does\n",
    "\n",
    "* **Python & NumPy** – Many data-pipeline utilities (shuffling lists, image augmentations) rely on these random-number generators. Seeding them first removes one entire layer of randomness.\n",
    "* **PyTorch CPU / GPU** – `torch.manual_seed` covers every op executed on the CPU, while `torch.cuda.manual_seed_all` applies the same seed to each GPU stream so that multi-GPU jobs stay in sync.\n",
    "* **cuDNN flags** – By default cuDNN picks the fastest convolution algorithm, which can vary run-to-run. Setting `deterministic=True` forces repeatable kernels and turning `benchmark` *off* prevents the auto-tuner from replacing those kernels mid-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `set_seed()`\n",
    "\n",
    "Running the command *before* you build models, create datasets, or start data-loader workers guarantees that every downstream object inherits the same seed.  If you call it later, some layers or tensors may already have been initialised with non-deterministic values, breaking repeatability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} — main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now know how to fix the seed for reproducibility. Now, let's answer the following question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why is random initialization useful for the model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (1970201198.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    By initializing the weights to small, random numbers, you break this symmetry. Each neuron starts from a different point, allowing it to follow a unique path during training and learn to detect a different feature. This diversity is crucial for the network's ability to learn complex patterns and ultimately converge to an effective solution.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "<!--\n",
    "Random initialization is useful because it ensures that each neuron in the network starts with different weight values, which is essential for effective learning and convergence.\n",
    "\n",
    "The Importance of Breaking Symmetry\n",
    "When training a neural network, if all the weights were initialized to the same value (e.g., zero), every neuron in a given layer would perform the exact same calculation. This means they would all have the same output and receive the same gradient during backpropagation. As a result, all the weights would update identically in every training step.\n",
    "\n",
    "This symmetry prevents the neurons from learning different features from the data, making them redundant. It's like having a team where every member has the exact same idea and skill—they can't effectively tackle a complex problem.\n",
    "\n",
    "By initializing the weights to small, random numbers, you break this symmetry. Each neuron starts from a different point, allowing it to follow a unique path during training and learn to detect a different feature. This diversity is crucial for the network's ability to learn complex patterns and ultimately converge to an effective solution.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "Random initialization, each neuron starts differently, enabling effective learning and convergence.    \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `worker_init_fn` function\n",
    "\n",
    "PyTorch offsets each worker’s seed by default, injecting new randomness. For reproducible results, you want workers to start from **fixed** seeds so every data-augmentation decision (flip, crop, colour-jitter) is repeatable across runs. The `worker_init_fn` function re-seeds Python, NumPy, and PyTorch CPU random-number generators inside **each** worker using a simple deterministic formula (`SEED + worker_id`).  The result will be identical batches, identical gradients, and identical model checkpoints, run after run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id: int) -> None:\n",
    "    \"\"\"Re-seed each DataLoader worker so their RNGs don't collide.\"\"\"\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed) \n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining `dataset_path`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters and device\n",
    "\n",
    "You have to define the key **hyperparameters** that control the model's training process. Hyperparameters are set by the user to configure the learning algorithm.\n",
    "\n",
    "- **`img_size`**: The spatial resolution (height and width) to which all images will be resized. This ensures that the input to the neural network is of a consistent size.\n",
    "- **`batch_size`**: The number of training examples utilized in one iteration (one forward and backward pass). A larger batch size can lead to faster training but requires more memory.\n",
    "- **`lr` (Learning Rate)**: A crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\n",
    "- **`epochs`**: The number of times the learning algorithm will work through the entire training dataset.\n",
    "- **`model_name`**: The name of the model file that will be created after training. This is useful for saving the checkpoint while training.\n",
    "- **`device`**: This line programmatically checks if a CUDA-enabled GPU is available using `torch.cuda.is_available()`. If a GPU is found, the device is set to `\"cuda\"` to leverage hardware acceleration. Otherwise, it defaults to the `\"cpu\"`. This makes the code portable and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "model_name = \"ai_capstone_pytorch_state_dict.pth\"\n",
    "num_classes = 2 #number of classes in the dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device used is {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data pipeline\n",
    "You have downloaded the dataset and fixed the initial random seed for reproducibility. Now, you can start to build the data pipeline to feed data for training the model.\n",
    "To create the data pipeline for PyTorch, you will:\n",
    "1. Define transformations\n",
    "2. Split the dataset for training and validation\n",
    "3. Create the dataloader to feed the data into the training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transformations\n",
    "Here, you will define a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "This cell constructs the entire pipeline for loading and preparing the image data for the model. It involves defining transformations, splitting the data, and creating data loaders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create the training transformation pipeline `train_transform` using the `tranforms.Compose` . \n",
    "You may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size, useful for input vector with fixed dimensions for model training\n",
    "- `transforms.RandomRotation`: For geometrical rotation\n",
    "- `transforms.RandomHorizontalFlip`: For Geometrical horizontal flipping\n",
    "- `transforms.RandomAffine`: For adjusting to a different point-of-view\n",
    "\n",
    "Then, convert the image array to a Tensor using `transforms.ToTensor()`.\n",
    "\n",
    "And finally, normalize the images between [-1,1] using `transforms.Normalize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assuming 'img_size' is a predefined variable, for example:\n",
    "img_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, shear=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"train_transform pipeline created successfully:\")\n",
    "print(train_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`train_transform`**: This defines a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "- **`val_transform`**: The transformation for the validation set is simpler. It omits the random augmentation steps because you want to evaluate the model's performance on the original, unaltered data.\n",
    "- **`datasets.ImageFolder`**: This PyTorch utility automatically loads an image dataset from a directory where subdirectories are named after their corresponding classes (e.g., `data/agri`, `data/non_agri`).\n",
    "- **`random_split`**: The full dataset is partitioned into training (80%) and validation (20%) sets. This separation is crucial for assessing how well the model generalizes to unseen data.\n",
    "- **`DataLoader`**: These objects wrap the datasets and provide an efficient, iterable way to feed data to the model in batches. `shuffle=True` for the `train_loader` ensures that the model sees the data in a different order each epoch, which helps prevent it from learning the order of the training examples. `worker_init_fn` ensures that **fixed seed** is passed to the `dataloader` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create the validation transformation pipeline `val_transform`.\n",
    "The validataion dataset is just for validating the preformace of the model and hence, doesn't need to augment the input images. \n",
    "So, you may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size\n",
    "- `transforms.ToTensor()`\n",
    "-  `transforms.Normalize`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assuming 'img_size' is a predefined variable, for example:\n",
    "img_size = 64\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"val_transform pipeline created successfully:\")\n",
    "print(val_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "You have defined the transformation pipelines for the training and validation datasets. \n",
    "\n",
    "Next, you will use the `datasets.ImageFolder` utility to load an image dataset from the root directory `dataset_path`. \n",
    "\n",
    "This root directory contains the subdirectories where each subdirectory corresponds to a class (e.g., `data/agri`, `data/non_agri`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split: Train and validation\n",
    "\n",
    "The next step in the data loading pipeline is to split the image dataset for training and validation. \n",
    "\n",
    "You can use `random_split` from `torch.utils.data` class. \n",
    "\n",
    "This method allows you to randomly split the input data based on a pre-defined split ratio for the training and validation datasets. \n",
    "\n",
    "In this case, you can use 80% (0.8) dataset for training and 20% (0.20) for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataLoader\n",
    "\n",
    "Now, you can use the `DataLoader` from `torch.utils.data` class to create a dataset generator for lazy loading of the training dataset.\n",
    "In the input, you define \n",
    "- `train_dataset`: The training image dataset\n",
    "- `batch_size`: The number of images to be loaded in each batch\n",
    "- `shuffle`: Set to *True* to load images from the dataset in random order\n",
    "- `num_workers`: Number of parallel processes used to load the images. This is for optimum utilization of your CPU cores to reduce the image I/O bottleneck\n",
    "- `worker_init_fn`: For function to decide on data augmentation. The default is with *random seed* for better generalization or *fixed seed* for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          worker_init_fn=worker_init_fn\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation DataLoader\n",
    " \n",
    "Now that you know how to create the train dataloader, in this step, you will create a validation step dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: create `val_loader` for the validation dataset\n",
    "\n",
    "You have to create the validation dataloader `val_loader` for validation of model in each training step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the following variables are already defined:\n",
    "# val_dataset: The validation dataset object\n",
    "# batch_size: The number of samples per batch\n",
    "# worker_init_fn: The function to initialize worker processes\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        worker_init_fn=worker_init_fn\n",
    "                       )\n",
    "\n",
    "print(\"Validation DataLoader created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        worker_init_fn=worker_init_fn\n",
    "                       )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created Dataloaders. Now creating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "You will define the Convolutional Neural Network (CNN) architecture and configure the components needed for training.\n",
    "<p></p>\n",
    "\n",
    "\n",
    "<p></p>\n",
    "\n",
    "- **`model = nn.Sequential(...)`**: A sequential container is used to build the model as a linear stack of layers. This is a convenient way to define a straightforward CNN.\n",
    "  - **Convolutional Blocks**: The model consists of several blocks, each containing\n",
    "      - a `Conv2d` layer for feature extraction,\n",
    "      - a `ReLU` activation function,\n",
    "      - a `MaxPool2d` layer to downsample and reduce dimensionality,\n",
    "      - a`BatchNorm2d` to stabilize and accelerate training.    \n",
    "  - **Classifier**: After the convolutional blocks,\n",
    "      - `AdaptiveAvgPool2d` reduces each feature map to a single value, making the model more robust to input size variations.\n",
    "      - `Flatten` converts the 2D feature maps into a 1D vector.\n",
    "      - `Linear` (fully connected) layers then perform the final classification,\n",
    "      - `Dropout` is used as a regularization technique to prevent overfitting.\n",
    "  - **`.to(device)`**: This moves the model's parameters and buffers to the selected device (GPU, if available otherwise CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "model = nn.Sequential(\n",
    "                        # Conv Block 1\n",
    "                        nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "                        nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "                        \n",
    "                        # Conv Block 2-6\n",
    "                        nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "                        nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "                        nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "                        \n",
    "                        # Classifier\n",
    "                        nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "                        nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "                        nn.Linear(2048, num_classes)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training setup\n",
    "\n",
    "After defining the model, you declare the loss function and the optimizer for backpropagation and learning\n",
    "You also set up the tracking of the history of the model training for loss and accuracy for every step of the model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function is defined using **`criterion = nn.CrossEntropyLoss()`**\n",
    "    - `CrossEntropyLoss` is specifically designed for multi classs classification problems.\n",
    "<p></p>\n",
    "<p></p>\n",
    "- The optimizer is defined using **`optimizer = optim.Adam(...)`**:\n",
    "    - The Adam optimizer is chosen to update the model's weights. It's an adaptive learning rate method that is computationally efficient and works well in practice.\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- You can **track the history** using `best_loss`, `loss_history` and `acc_history` dictionaries\n",
    "    - `best_loss`: stores the best validation loss achieved so far.\n",
    "    - `loss_history` and `acc_history` dictionaries to log the loss and accuracy history for plotting later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRAINING SETUP ---\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "best_loss = float('inf')\n",
    "loss_history = {'train': [], 'val': []}\n",
    "acc_history = {'train': [], 'val': []}\n",
    "\n",
    "print(\"Created Model. Now training the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation\n",
    "\n",
    "Your neural network is now ready for training.\n",
    "\n",
    "Here, you will set up the main logic for how the model learns from the data. The model iterates through the dataset for the specified number of epochs, with each epoch consisting of a training phase and a validation phase.\n",
    "\n",
    "- **Outer Loop (`for epoch in range(epochs)`)**: Controls the number of full passes over the dataset.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "\n",
    "- **In the interest of time, we are training the model for just 3 epochs**.\n",
    "    - Generally, you train a model for many more epochs (atleast 15, usually). The model trained for 20 epochs can be found **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Training Phase**:\n",
    "  - `model.train()`: Sets the model to training mode. This activates layers like Dropout and ensures BatchNorm layers learn from the current batch statistics.\n",
    "  - **Inner Loop (`for images, labels in train_loader`)**: Iterates over batches of training data.\n",
    "  - `optimizer.zero_grad()`: Clears the gradients from the previous iteration before computing new ones.\n",
    "  - `outputs = model(images)`: **Forward Pass**. The input data is passed through the network to get predictions (logits).\n",
    "  - `loss.backward()`: **Backward Pass**. Gradients of the loss with respect to the model's parameters are calculated.\n",
    "  - `optimizer.step()`: The optimizer updates the model's parameters using the computed gradients.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Validation Phase**:\n",
    "  - `model.eval()`: Sets the model to evaluation mode. This deactivates Dropout and makes BatchNorm layers use their learned running statistics.\n",
    "  - `with torch.no_grad()`: Disables gradient calculation, which speeds up computation and reduces memory usage since you are only evaluating, not training.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Model Checkpointing**: After each epoch, the current validation loss is compared to the `best_loss` seen so far. If the current loss is lower, the model's state (`model.state_dict()`) is saved to a file. This ensures that you keep the model version that performed best on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    start_time = time.time() # to get the training time for each epoch\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0  # for the training metrics\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "        images, labels = images.to(device), labels.to(device)  # labels as integer class indices\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # outputs are raw logits\n",
    "        loss = criterion(outputs, labels)  # criterion is nn.CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "  \n",
    "    # Synchronize CUDA before stopping timer (if using GPU)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0 #  for the validation metrics\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "  \n",
    "    # Save the best model\n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "    \n",
    "    # Store metrics\n",
    "    loss_history['train'].append(train_loss/len(train_loader))\n",
    "    loss_history['val'].append(val_loss/len(val_loader))\n",
    "    acc_history['train'].append(train_correct/train_total)\n",
    "    acc_history['val'].append(val_correct/val_total)\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {loss_history['train'][-1]:.4f} | Val Loss: {loss_history['val'][-1]:.4f}\")\n",
    "    print(f\"Train Acc: {acc_history['train'][-1]:.4f} | Val Acc: {acc_history['val'][-1]:.4f}\")\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} training completed in {epoch_time:.2f} seconds\\n\") \n",
    "\n",
    "print(\"Trained Model. Now evaluating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully trained the model using PyTorch libraries. As you can see, during the model training, each step is easy accessible for fine tuning the model. This gives the user an advantage by giving them control over every hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training cell above, please answer the following questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is `tqdm` used for?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this cell to type the answer to the question.\n",
    "<!--\n",
    "The tqdm library is used to provide a progress bar for monitoring the progress of an operation, such as a training epoch. ⏳\n",
    "\n",
    "Visualizing Progress\n",
    "In the provided notebook, tqdm is wrapped around the train_loader during the training loop. This instantly creates a smart progress bar that displays useful information, including:\n",
    "\n",
    "How many batches have been processed out of the total.\n",
    "\n",
    "The time elapsed.\n",
    "\n",
    "The estimated time remaining for the current epoch.\n",
    "\n",
    "The rate of iteration (batches per second).\n",
    "\n",
    "This visual feedback is very helpful for long-running processes like training a neural network, as it allows you to gauge the training speed and know that the process hasn't stalled.\n",
    "    -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"The tqdm library is used to provide a progress bar to monitor the progress of each epoch.\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why are the `train_loss`, `train_correct` and `train_total` set to 0 in every epoch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "<!--\n",
    "These variables are reset to zero at the start of each epoch because their purpose is to accumulate metrics for that specific epoch only.\n",
    "\n",
    "Per-Epoch Metric Calculation\n",
    "Think of train_loss, train_correct, and train_total as accumulators or counters for a single training cycle. 🔄\n",
    "\n",
    "An epoch is defined as one complete pass through the entire training dataset. To measure how well the model is performing during a particular epoch, you need to calculate metrics like the average loss and accuracy for just that pass.\n",
    "\n",
    "Here's how it works in the training loop:\n",
    "\n",
    "Reset: At the beginning of a new epoch, the counters are set to zero to ensure a clean slate.\n",
    "\n",
    "Accumulate: As the model processes each batch of data within the epoch, the loss, the number of correct predictions, and the total number of samples are added to these variables.\n",
    "\n",
    "Calculate: At the end of the epoch, the final accumulated values are used to calculate the average loss and overall accuracy for that specific epoch.\n",
    "\n",
    "If these variables were not reset, the metrics would continue to accumulate across all epochs, and you would lose the ability to see how the model's performance changes from one epoch to the next.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"Because they accumulate metrics for that specific epoch only\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why do you need to use `torch.no_grad()` in the validation loop?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "<!--\n",
    "You use torch.no_grad() in the validation loop to disable gradient calculation, which is unnecessary for evaluation and improves performance.\n",
    "\n",
    "Performance and Memory Optimization ⚡️\n",
    "During the training phase, PyTorch builds a computation graph that tracks every operation. This graph is essential for backpropagation, where gradients are calculated to update the model's weights.\n",
    "\n",
    "However, during validation, the goal is only to evaluate the model's performance on the data, not to update its weights. Therefore, calculating and storing gradients is a waste of resources.\n",
    "\n",
    "By wrapping the validation loop in a with torch.no_grad(): context manager, you tell PyTorch to temporarily turn off gradient calculations. This provides two key benefits:\n",
    "\n",
    "Faster Computation: Without the need to track operations for the backward pass, the forward pass runs significantly faster.\n",
    "\n",
    "Reduced Memory Usage: The computation graph and the resulting gradients consume a lot of memory. Disabling them frees up this memory, which can be crucial when working with large models or limited GPU memory.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"It disables gradient calculation as you do not need gradient calculation for validation\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are two different metrics on which the model can be evaluated for best performance during training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "<!--\n",
    "Two common metrics for evaluating a model's best performance during training are validation loss and validation accuracy.\n",
    "\n",
    "Validation Loss \n",
    "Validation loss measures the model's error on the validation dataset—data it doesn't see during the training phase. It quantifies how \"wrong\" the model's predictions are. A lower validation loss indicates a better-performing model. In the notebook's training loop, the model's state is saved whenever the average validation loss for an epoch is lower than the best loss recorded so far. This makes it a primary metric for model checkpointing.\n",
    "\n",
    "Validation Accuracy \n",
    "Validation accuracy is the percentage of correct predictions the model makes on the same unseen validation dataset. It's a more direct and intuitive measure of the model's classification performance. A higher validation accuracy indicates a better model. While validation loss is often used to guide the saving of the best model, validation accuracy gives a clear, real-world sense of how effectively the model is learning to classify the data.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"the validation loss and validation accuracy\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `torch.save` function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model file: `ai_capstone_pytorch_state_dict.pth` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- This model could be used in other labs of this AI capstone course, instead of the model provided at the above link\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the file to your local computer.\n",
    "\n",
    "\n",
    "### The trained model state file (`ai_capstone_pytorch_state_dict.pth` ) in the left pane\n",
    "![Model_PyTorch_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Nar8kA3Qrz3uCmFtFrKI9g/Model-PyTorch-download-screenshot-1-marked.png)\n",
    "\n",
    "### The **download** option\n",
    "![Model_PyTorch_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HKO5ROsE1erbqcE6Kq8ysA/Model-PyTorch-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training history\n",
    "\n",
    "Here, you can use `matplotlib` to create plots of the model's accuracy and loss over each epoch. Visualizing these metrics is useful for understanding the training dynamics.\n",
    "\n",
    "\n",
    "Usually, the following two plots are used to track the training history of a model:\n",
    "- **Accuracy Plot**: Shows the training accuracy versus the validation accuracy. A large gap between the two curves can be an indicator of overfitting, where the model performs well on the data it has seen but poorly on new, unseen data.\n",
    "\n",
    "- **Loss Plot**: Shows the training loss versus the validation loss. An ideal plot shows both losses decreasing and converging. If the validation loss starts to increase while the training loss continues to decrease, it's a strong sign of overfitting.\n",
    "\n",
    " \n",
    "These plots provide an intuitive, visual summary of the entire training process and help diagnose potential issues or confirm that the model has trained successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the **Model Accuracy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(acc_history['train'], label='Train Acc')\n",
    "plt.plot(acc_history['val'], label='Val Acc')\n",
    "plt.title('Model Accuracy (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write the code for the loss plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Plot the *Model Loss* from the training history of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'loss_history' is a dictionary containing the training history\n",
    "# loss_history = {'train': [loss_epoch_1, ...], 'val': [val_loss_epoch_1, ...]}\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history['train'], label='Train Loss')\n",
    "plt.plot(loss_history['val'], label='Val Loss')\n",
    "plt.title('Model Loss (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history['train'], label='Train Loss')\n",
    "plt.plot(loss_history['val'], label='Val Loss')\n",
    "plt.title('Model Loss (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "This cell comprehensively evaluates the best-performing model saved during the training loop. While accuracy provides a high-level view, these metrics provide a deeper insight into the model's behavior.\n",
    "\n",
    "\n",
    "- **`model.eval()`**: Switches the model to evaluation mode.\n",
    "- **`with torch.no_grad()`**: Disables gradient computation for efficiency.\n",
    "- **Collecting Predictions**: The code iterates through the entire validation set to gather all predictions and their corresponding true labels.\n",
    "\n",
    "- **`accuracy`**: The proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, you have to get the predictions for the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: For the images from `val_loader`, get a list of :\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "**1.** all predictions `all_preds`\n",
    " \n",
    "**2.** the ground truth labels `all_labels` \n",
    "\n",
    "For the predictions, you will have to move the data to the CPU using `predictions.cpu()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "import torch\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # Loop through each batch in the validation dataloader\n",
    "    for images, labels in val_loader:\n",
    "        # Move images to the same device as the model\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get model predictions (forward pass)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get the predicted class by finding the index of the max logit\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Move predictions to CPU, convert to NumPy array, and add to the list\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        \n",
    "        # Convert labels to a NumPy array and add to the list\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Finished collecting all predictions and labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.numpy())\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of the plot\n",
    "\n",
    "**`accuracy`** is the proportion of correct predictions out of the total predictions.\n",
    "\n",
    "You can use `accuracy_score` from `sklearn.metrics` class to calculate the **accuracy_score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"The accuracy of the model is: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully built, trained, and evaluated a deep learning model for image classification using PyTorch.\n",
    "\n",
    "In this lab, you coded for:\n",
    "- **Data loading pipeline:** Implementing ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** Building a multi-layered CNN incorporating Conv2D layers.\n",
    "- **Model training setup:** Configuring the model’s learning process using the Adam optimizer, BCEWithLogitsLoss, and tracking accuracy and loss metrics.\n",
    "- **Training process:** Executing the training loop by feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** Plotting accuracy and loss curves to analyze learning progress and detect overfitting.\n",
    "- **Model evaluation:** Assessing model performance using accuracy_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-04  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "0438099ac7cecf8186ed65f816edd2dc3553a779fcf8536358210af38290d216"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
